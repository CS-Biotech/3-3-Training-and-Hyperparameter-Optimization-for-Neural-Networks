{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 4: Training and Hyperparameter Optimization for Neural Networks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Welcome to Week 4! Last week, we built the foundational components of our machine learning pipeline: preparing the dataset, creating a simple model structure, and setting up the basics of training and evaluation for a basic neural network. This week, we’ll take a deeper look at the **training process itself** — the engine that allows our model to actually learn.\n",
    "\n",
    "## Learning objectives\n",
    "\n",
    "* Understand the role of **loss functions** and why they are central to guiding model learning.\n",
    "* Explore different **optimization algorithms** (e.g., SGD, Adam) and how they update model parameters.\n",
    "* Learn how to track and interpret **training vs. validation loss** curves.\n",
    "* Gain hands-on experience adjusting **hyperparameters** (learning rate, batch size, epochs) and seeing their effect on performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip -q install nbimporter torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of having all our helper functions in this notebook, we will place it into `utils.py` and reference it when needed. Take a minute to review the functions and classes in `utils.py`. Make note of the familiar classes and functions from Week 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import train_model, evaluate_model, plot_loss, get_generic_model, get_pseudo_predictions, get_pseudo_targets, plot_pseudo_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Introduction to the Training Process\n",
    "\n",
    "In this section, you will be introduced to the core mechanics of training a neural network. This includes the concepts of epochs, loss landscapes, and the goals of optimization. Training a neural network involves minimizing a loss function, which can be visualized as navigating a landscape with peaks and valleys. A global minimum represents the best possible performance, while local minima represent suboptimal solutions where training can get stuck.\n",
    "\n",
    "![minima](minima.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall the training loop from Week 3. This is what the actual function looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, criterion=None, optimizer=None, learning_rate=1e-3, num_epochs=30, seed=42):\n",
    "    \"\"\"\n",
    "    Function to train the model and record loss history.\n",
    "    \"\"\"\n",
    "    # Set the default loss function\n",
    "    if criterion is None:\n",
    "        criterion = nn.MSELoss()\n",
    "    \n",
    "    # Set the default optimizer\n",
    "    if optimizer is None:\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Store loss history for plotting\n",
    "    history = {'train_loss': [], 'val_loss': []}\n",
    "    \n",
    "    print(\"Starting training and validation...\")\n",
    "    for epoch in range(num_epochs):\n",
    "        \n",
    "        # Training Phase\n",
    "        model.train()  # Set the model to training mode\n",
    "        running_train_loss = 0.0\n",
    "        \n",
    "        for inputs, targets in train_loader:\n",
    "            optimizer.zero_grad()               # Clear previous gradients\n",
    "            outputs = model(inputs)             # Forward pass (get predictions)\n",
    "            loss = criterion(outputs, targets)  # Calculate loss\n",
    "            loss.backward()                     # Backward pass (compute gradients)\n",
    "            optimizer.step()                    # Update weights with gradients\n",
    "            \n",
    "            running_train_loss += loss.item() * inputs.size(0)\n",
    "            \n",
    "        epoch_train_loss = running_train_loss / len(train_loader.dataset)\n",
    "        history['train_loss'].append(epoch_train_loss)\n",
    "        \n",
    "        # Validation Phase\n",
    "        model.eval()  # Set the model to evaluation mode\n",
    "        running_val_loss = 0.0\n",
    "        \n",
    "        with torch.no_grad():  # Turns off gradient tracking during evaluation\n",
    "            \n",
    "            for inputs, targets in val_loader:\n",
    "                \n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "                running_val_loss += loss.item() * inputs.size(0)\n",
    "                \n",
    "        epoch_val_loss = running_val_loss / len(val_loader.dataset)\n",
    "        history['val_loss'].append(epoch_val_loss)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} | \"\n",
    "              f\"Train Loss: {epoch_train_loss:.4f} | \"\n",
    "              f\"Val Loss: {epoch_val_loss:.4f}\")\n",
    "        \n",
    "    print(\"Finished Training.\")\n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Components of the `train_model()` Function\n",
    "\n",
    "Let us walk through some key components of the `train_model()` function. \n",
    "\n",
    "- `criterion` (Loss): The `criterion` is our loss function. The `criterion` measures how far the model's predictions are from the ground truth values. The goal of training is to minimize this loss. Here, we use the **Mean Squared Error (MSE)** loss function.\n",
    "\n",
    "- `optimizer`: The `optimizer` updates the model's weights to reduce the loss. This function uses the **Adam** optimizer, a widely used, adaptive method that performs well across many tasks with default settings. Think of it as the “coach” that decides how big or small the corrections should be after each mistake.\n",
    "\n",
    "    * It looks at the **gradients** (calculated during backpropagation) and nudges the parameters in the direction that should reduce error.\n",
    "    * Examples: **SGD**, **Adam**, **RMSProp**.\n",
    "\n",
    "- `history`: The `history` dictionary stores the training and validation loss after each pass through the dataset. This enables us to plot learning curves, monitor progress, and diagnose issues such as overfitting. Its structure also works directly with the `data_loaders.plot_loss()` function.\n",
    "\n",
    "- Training and Validation loops: The loops repeatedly train and evaluate the model over multiple full passes through the training data. Before each phase, we set the model mode with `model.train()` and `model.eval()`, which ensures that certain special layers (like dropout or batch normalization) behave correctly during training and evaluation.\n",
    "\n",
    "Below, we will elaborate on some of these key concepts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Training Phase\n",
    "\n",
    "The **<span style=\"background-color: #AFEEEE\">**training phase**</span>** is a repetitive process where a neural network learns from data. Each full pass through all the training examples is called an **<span style=\"background-color: #AFEEEE\">**epoch**</span>**. During an epoch, the model makes predictions, measures error, and updates its parameters to reduce the loss. Over many epochs, the model gradually improves --- much like practicing a skill: you make a guess, see the error, adjust, and try again.\n",
    "\n",
    "Within each epoch, we process the training data in **batches**, performing the following steps for each batch:\n",
    "\n",
    "1. **Forward Pass**: Use the current model parameters to make predictions.\n",
    "2. **Calculate Loss**: Measure how far the current predictions are from the ground truth values.\n",
    "3. **Backward Pass**: Determines how each weight contributed to the error. The resulting gradients indicate how to adjust the weights to reduce the error.\n",
    "4. **Weight Update**: Call `optimizer.step()` to adjust the weights based on the gradients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backpropagation\n",
    "\n",
    "The series of steps described above (including forward pass and backward pass) is sometimes called the **<span style=\"background-color: #AFEEEE\">**backpropagation**</span>** algorithm. It refers to an efficient process to compute the gradients for all the weights in the neural network through one full backward pass from the outputs to the inputs. Behind the scenes, the algorithm computes gradients using calculus, and the gradients indicate how each weight should change to reduce the loss. \n",
    "\n",
    "You don't need to worry about the math.  The key idea to remember is that backpropagation tells the model how to adjust its weights to improve its predictions. If you're interested in learning more, take a look at [this resource](https://www.youtube.com/watch?v=Ilg3gGewQ5U&embeds_referring_euri=https%3A%2F%2Fchatgpt.com%2F&source_ve_path=MjM4NTE)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Validation Phase\n",
    "\n",
    "The **<span style=\"background-color: #AFEEEE\">**validation phase**</span>** is used to evaluate how well the neural network generalizes to unseen data. Unlike training, this phase does **not** update the model’s weights. Instead, it measures performance on a separate **validation set** after each epoch to track progress and \"validate\" the model's progress.\n",
    "\n",
    "During validation, the following steps occur:\n",
    "\n",
    "1. **Forward Pass:** The model makes predictions on the validation data using the current weights.\n",
    "2. **Calculate Loss:** The loss is computed to estimate how far these predictions are from the true labels, just like in training — but without computing gradients.\n",
    "3. **Performance Metrics:** Accuracy, precision, recall, or other metrics are calculated to assess model quality.\n",
    "4. **No Backpropagation:** The model’s parameters are **not** updated — gradients are disabled to save memory and ensure the evaluation reflects the model’s current state.\n",
    "\n",
    "By comparing **training** and **validation** loss over epochs, we can see whether the model is learning effectively, underfitting, or overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Function\n",
    "\n",
    "In machine learning, a **<span style=\"background-color: #AFEEEE\">**loss function**</span>** measures how wrong a model’s predictions are by comparing them to the true values. It outputs a single number—the **loss**—which indicates the model’s prediction error. A lower loss means better performance.\n",
    "\n",
    "During training, the model adjusts its parameters to **minimize this loss**. It uses **backpropagation** to compute how much each parameter contributed to the loss, and an **optimizer** (like SGD or Adam) updates the parameters to reduce the error in future predictions.\n",
    "\n",
    "The loss function is critical because it **defines the goal of learning**—guiding the model on what \"improvement\" means. Without it, the model wouldn’t know whether it’s getting better or worse.\n",
    "\n",
    "Different tasks require different loss functions. For example:\n",
    "\n",
    "* **Mean Squared Error (MSE)** is commonly used for regression problems.\n",
    "* **Cross-Entropy Loss** is used for classification tasks.\n",
    "\n",
    "#### Bullseye Metaphor\n",
    "\n",
    "Imagine the true answer is the **center of a bullseye target**.\n",
    "\n",
    "* When the model’s prediction is **right on the bullseye**, the loss is **zero** — perfect accuracy.\n",
    "* If the prediction **lands close to the center**, the loss is **small**, meaning the model did well but not perfectly.\n",
    "* If the prediction **hits far from the bullseye**, the loss is **large**, showing a bigger mistake.\n",
    "* During training, the model tries to “aim its arrow” closer and closer to the bullseye to reduce the loss.\n",
    "\n",
    "The loss function measures the **distance from the bullseye**, guiding the model to improve its aim with each try. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing a Loss Function using `criterion()`\n",
    "\n",
    "In `PyTorch`, the term `criterion` typically refers to the **loss function object**. It's defined before training starts and used to calculate the loss during each training step. \n",
    "\n",
    "For example:\n",
    "\n",
    "> `import torch.nn as nn`\n",
    "> \n",
    "> `criterion = nn.CrossEntropyLoss()`  # For classification\n",
    "\n",
    "You then use it like this during training:\n",
    "\n",
    "> `loss = criterion(predictions, targets)`\n",
    "\n",
    "Here, `criterion()` takes the model’s predictions and the true labels, and returns the loss value. This value is then used in backpropagation:\n",
    "\n",
    "> `loss.backward()`\n",
    "> \n",
    "> `optimizer.step()`\n",
    "\n",
    "So, the `criterion()` is the functional tool that connects **predictions**, **ground truth**, and the **learning objective**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Underfitting vs. Overfitting\n",
    "\n",
    "As the model learns, it’s important to make sure it generalizes well—not just memorizing training data. We touched on underfitting and overfitting in previous modules such as HMB201 Week 7. Here is a quick refresher:\n",
    "\n",
    "* **Underfitting** happens when a model is not able to capture the useful/important patterns in the dataset, leading to poor performance on both training and test data.\n",
    "  *Example:* A straight line trying to fit a curved pattern (refer to left most graph in the image below).\n",
    "\n",
    "* **Overfitting** happens when a model performs poorly on unseen test data, even though it performed well during training and performs well on training data but poorly on new, unseen data such as the test and validation set.\n",
    "  *Example:* A wiggly curve that fits every training point exactly but fails on test data (refer to right most graph in the image below).\n",
    "![fit examples](fit_examples.png)\n",
    "\n",
    "Image retrieved from [here](https://medium.com/greyatom/what-is-underfitting-and-overfitting-in-machine-learning-and-how-to-deal-with-it-6803a989c76).\n",
    "\n",
    "The loss function and `criterion()` help monitor these issues:\n",
    "\n",
    "* A very low training loss but high validation loss indicates **overfitting**.\n",
    "* A high loss on both training and validation sets suggests **underfitting**.\n",
    "* A low training loss AND low validation loss, with both being close to each other suggests a well fitted model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try an exercise to visualize the targets, prediction and criterion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Mean Squared Error Loss\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Fake \"model outputs\" (predictions)\n",
    "predictions = get_pseudo_predictions()\n",
    "\n",
    "# Fake \"ground truth\" targets\n",
    "targets = get_pseudo_targets()\n",
    "\n",
    "# Calculate loss\n",
    "loss_value = criterion(predictions, targets)\n",
    "\n",
    "print(f\"Predictions:\\n{predictions}\")\n",
    "print(f\"Targets:\\n{targets}\")\n",
    "print(f\"Loss: {loss_value.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pseudo_data(predictions, targets, loss_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will give you:\n",
    "\n",
    "- Blue dots = model predictions\n",
    "\n",
    "- Green dots = actual targets\n",
    "\n",
    "- Red dashed lines = error for each point\n",
    "\n",
    "**Formula:**\n",
    "\n",
    "$$\n",
    "\\text{MSE} = \\frac{1}{n} \\sum_{i=1}^n (prediction_i - target_i)^2\n",
    "$$\n",
    "\n",
    "So in this case:\n",
    "\n",
    "* Prediction: 0.2, Target: 0.0 → Error: 0.2 → Squared: 0.04\n",
    "* Prediction: 0.8, Target: 1.0 → Error: -0.2 → Squared: 0.04\n",
    "* Prediction: 0.5, Target: 0.6 → Error: -0.1 → Squared: 0.01\n",
    "* **Mean** = (0.04 + 0.04 + 0.01) / 3 = 0.03\n",
    "\n",
    "That `0.03` is what `criterion` returns — the **average squared difference between your predictions and the truth**.\n",
    "\n",
    "Note: the criterion can be set to other loss functions. Here are some common choices:\n",
    "\n",
    "- Regression → `MSELoss`, `L1Loss`, `HuberLoss`.\n",
    "\n",
    "- Binary classification → `BCEWithLogitsLoss`.\n",
    "\n",
    "- Multi-class classification → `CrossEntropyLoss`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Q*1: Choose 2 different `criterion` loss functions and run them with the pseudo predictions and targets. Print the losses.**\n",
    "> Hint: Refer to the documentation [here](https://docs.pytorch.org/docs/stable/nn.html#loss-functions)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"background-color: #FFD700\">**Write your code below**</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "\n",
    "criterion = ...\n",
    "\n",
    "# Calculate loss\n",
    "\n",
    "\n",
    "criterion = ...\n",
    "\n",
    "# Calculate loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the Loss Surface\n",
    "\n",
    "In machine learning, a **loss surface** is a visual or mathematical representation of how the **loss function** (which measures prediction error) changes based on the model's parameters.\n",
    "\n",
    "Definitions:\n",
    "\n",
    "* The **loss function** quantifies how well a model's predictions match the actual values (lower is better).\n",
    "* The **loss surface** is the shape created when plotting the loss values against different combinations of model parameters.\n",
    "\n",
    "Real models have thousands or millions of parameters, but the following are 2D and 3D examples.\n",
    "\n",
    "### 2D Case: Local vs Global Minimum\n",
    "\n",
    "A **model with one parameter** (e.g., $w$) will produce a **2D graph** of the loss surface.\n",
    "\n",
    "* **X-axis**: The single model parameter (e.g., $w$)\n",
    "* **Y-axis**: Loss value $\\mathcal{L}(w)$\n",
    "\n",
    "In a 2D scenario, we can visualize the **loss landscape** as a curve:\n",
    "\n",
    "![2D loss surface](2D_loss_surface.png)\n",
    "\n",
    "Image retrieved from [here](https://medium.com/aimonks/navigating-the-peaks-and-valleys-of-optimization-global-minimum-vs-25c05de6f69a).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **red dashed line** is the **global minimum** — the best loss value.\n",
    "\n",
    "The **green dashed line** is a **local minimum** — better than nearby points, but not optimal.\n",
    "\n",
    "---\n",
    "\n",
    "### 3D Case: A More Realistic Loss Landscape\n",
    "\n",
    "For a model with **two parameters** (say $w_1$ and $w_2$), the loss surface can be visualized in 3D:\n",
    "\n",
    "* **X-axis**: Parameter 1 ($w_1$)\n",
    "* **Y-axis**: Parameter 2 ($w_2$)\n",
    "* **Z-axis**: Loss value $\\mathcal{L}(w)$\n",
    "\n",
    "This results in a surface — possibly a bowl, ridge, or complex landscape — depending on the model and data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![loss function visualization](descent_2D_sphere.gif)\n",
    "\n",
    "Image retrieved from [here](https://egallic.fr/Enseignement/ML/ECB/book/gradient-descent.html).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The GIF shows a point rolling down along a smooth 3D bowl-shaped loss surface, illustrating how an optimization algorithm like gradient descent moves model parameters step-by-step to minimize loss and find the best solution.\n",
    "\n",
    "In training, your optimizer \"walks\" across this surface trying to find a valley (minimum loss).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "| **Concept**          | **Description**                                                                                                         |\n",
    "| -------------------- | ----------------------------------------------------------------------------------------------------------------------- |\n",
    "| **Epoch**            | One full pass through the training data                                                                                 |\n",
    "| **Loss**             | A measure of prediction error                                                                                           |\n",
    "| **Local Minimum**    | A point where loss is low relative to nearby points                                                                     |\n",
    "| **Global Minimum**   | The lowest possible point on the entire loss surface                                                                    |\n",
    "| **Loss Surface**     | A multi-dimensional shape representing how loss changes with different model parameters                                 |\n",
    "| **Loss Curve**       | Graph showing how loss decreases during training                                                                        |\n",
    "| **Optimizer**        | Algorithm that adjusts model parameters to minimize loss; includes SGD, Adam, RMSProp, etc.                             |\n",
    "| **Gradient Descent** | An optimization algorithm that updates model parameters in the opposite direction of the gradient                       |\n",
    "| **Learning Rate**    | Step size used in gradient descent; too high may overshoot minima, too low slows training                               |\n",
    "| **Underfitting**     | The model is too simple or hasn’t trained enough; performs poorly on all data                                           |\n",
    "| **Overfitting**      | The model learns the training data too well, including noise; poor on new data                                          |\n",
    "| **Well-Fitting**     | The model performs well on both training and validation data; generalizes effectively                                   |\n",
    "| **Validation Loss**  | Loss measured on data not used for training; indicates generalization                                                   |\n",
    "| **Training Loss**    | Loss measured on data used to train the model                                                                           |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Visualizing Training Performance\n",
    "\n",
    "Visualizing loss and accuracy across epochs helps identify training behavior, such as overfitting, underfitting, and convergence. It gives immediate insight into whether training is working as expected.\n",
    "\n",
    "### Exercises:\n",
    "\n",
    "1. **Plot Training Curves**\n",
    "\n",
    "   * Use `matplotlib` to plot loss and accuracy per epoch.\n",
    "   * Add labels, grid, and legend for clarity.\n",
    "\n",
    "2. **Analyze Curve Behavior**\n",
    "\n",
    "   * Identify points where loss plateaus or spikes.\n",
    "   * Discuss what these indicate about the training process.\n",
    "\n",
    "3. **Bonus Task:**\n",
    "\n",
    "   * Add validation accuracy and loss to the same plots.\n",
    "   * Interpret gaps between training and validation curves.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From Week 3, you were introduced to a loss graph. Here is an example of a loss graph:\n",
    "\n",
    "![loss graph](example_training_loss_over_epochs.png)\n",
    "\n",
    "Image retrieved from [here](https://www.geeksforgeeks.org/deep-learning/training-and-validation-loss-in-deep-learning/).\n",
    "\n",
    "The loss is a calculation dependent on the `criterion` selected. You can plot these graphs using `plot_loss(history)` in `utils.py`.\n",
    "\n",
    "Here are example training loss curves:\n",
    "\n",
    "![learning curve scenarios](learning_curves_scenarios.jpg)\n",
    "\n",
    "1. **Plateau in Loss** (top left) – The loss stops improving, possibly due to a local minimum or a low learning rate.\n",
    "2. **Spikes in Loss** (top right) – Irregular jumps in the loss, often caused by a high learning rate or noisy batches.\n",
    "   > Note 1: small spikes = normal, large persistent spikes = check your setup.\n",
    "   > \n",
    "   > Note 2: not necessarily a bad thing (depends on your model and training setup).\n",
    "3. **Healthy Convergence** (bottom left) – A smooth, steady decrease in loss, showing effective learning.\n",
    "4. **Underfitting** (bottom right) – Loss decreases only slightly, indicating the model is too simple or undertrained.\n",
    "\n",
    "⚠️ **Important Note:**\n",
    "Not every loss curve pattern automatically means something is *bad*. For example, plateaus may just mean convergence, small spikes can be normal with noisy data, and even underfitting might be acceptable if your goal is simplicity. What matters most is interpreting these patterns in the **context of your dataset, model, and training objectives**. Loss curves are clues, not verdicts.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing Train vs Validation\n",
    "\n",
    "* If **training loss continues to decrease** but **validation loss increases**, it indicates **overfitting**.\n",
    "* A **large gap between train and val accuracy** also suggests overfitting — the model isn’t generalizing well.\n",
    "* **Small and stable gaps** between training and validation suggest the model is learning general patterns effectively.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Q*2: For each of the images below (left, middle, right), make an educated prediction of whether the graph depicts an underfitted, overfitted, or well-fitted model. Provide a reasoning for each prediction you make.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![fit](fit.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"background-color: #FFD700\">**Write your answer below**</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer here:\n",
    "* Left: \n",
    "* Middle: \n",
    "* Right: \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why use multiple epochs?\n",
    "\n",
    "In machine learning, **an epoch** is **one complete pass through the entire training dataset** by the learning algorithm.\n",
    "\n",
    "Training a model with only one pass (one epoch) is usually **not enough**. The model needs to **see the data multiple times** to learn the patterns well.\n",
    "\n",
    "Think of it like studying:\n",
    "\n",
    "* **One epoch** = reading the textbook once.\n",
    "* **Multiple epochs** = going over it several times to reinforce learning.\n",
    "\n",
    "### How many epochs to use?\n",
    "\n",
    "* Too **few** epochs → Underfitting (model hasn’t learned enough)\n",
    "* Too **many** epochs → Overfitting (model memorizes training data, doesn't generalize)\n",
    "\n",
    "> Common practice: Monitor **validation accuracy/loss** and stop training when performance plateaus (often using **early stopping**).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Q*3: Train a generic model with varying epochs. Use 3 different numbers for epoch settings and use `plot_loss()` for each test. Test your model using `evaluate_model()`**\n",
    "> Hint: Try using a for loop to reduce code repetition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTANT: Call get_generic_model() each time you start training.\n",
    "# This resets the model and optimizer configurations.\n",
    "# If you do not reset them, the model weights and optimizer state will carry over from previous runs.\n",
    "model, train_loader, val_loader, test_loader, criterion, optimizer = get_generic_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"background-color: #FFD700\">**Write your code below**</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "\n",
    "num_epochs = [...]\n",
    "\n",
    "...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Graded Exercise: (6 marks)** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**GQ*1: Why do we plot training and validation loss over epochs? (1 mark)**\n",
    "\n",
    "A. To measure how long training takes\n",
    "\n",
    "B. To track how well the model learns and generalizes\n",
    "\n",
    "C. To compare different datasets\n",
    "\n",
    "D. To check memory usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"background-color: #FFD700\">**Write your answer below**</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer here:\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**GQ*2: If training loss decreases but validation loss increases, what is happening? (1 mark)**\n",
    "\n",
    "A. Underfitting\n",
    "\n",
    "B. Overfitting\n",
    "\n",
    "C. Good generalization\n",
    "\n",
    "D. Data leak"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"background-color: #FFD700\">**Write your answer below**</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer here:\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**GQ*3: If both training and validation losses are high and flat, what does that mean? (1 mark)**\n",
    "\n",
    "A. The model is overfitting\n",
    "\n",
    "B. The model is underfitting\n",
    "\n",
    "C. The model is well-trained\n",
    "\n",
    "D. The learning rate is too low\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"background-color: #FFD700\">**Write your answer below**</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer here:\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**GQ*4: The training loss fluctuates wildly or suddenly increases. What’s the most likely cause? (1 mark)**\n",
    "\n",
    "A. Too few epochs\n",
    "\n",
    "B. Learning rate too high\n",
    "\n",
    "C. Too much regularization\n",
    "\n",
    "D. Dropout not applied\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"background-color: #FFD700\">**Write your answer below**</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer here:\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**GQ*5: After 10 epochs, both training and validation losses are still decreasing. What should you do? (1 mark)**\n",
    "\n",
    "A. Stop training \n",
    "\n",
    "B. Decrease the learning rate\n",
    "\n",
    "C. Train for more epochs\n",
    "\n",
    "D. Reduce model size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"background-color: #FFD700\">**Write your answer below**</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer here:\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**GQ*6: If the training and validation losses decrease and then level off close together, this suggests: (1 mark)**\n",
    "\n",
    "A. Underfitting\n",
    "\n",
    "B. Overfitting\n",
    "\n",
    "C. Good convergence\n",
    "\n",
    "D. Poor data quality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"background-color: #FFD700\">**Write your answer below**</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer here:\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this module, you explored the training process in more depth and learned how to interpret loss graphs. These skills are essential for understanding your model’s learning behavior, diagnosing issues like underfitting or overfitting, and identifying opportunities for improvement. You also learned how to evaluate model performance effectively—an important step in building reliable and accurate machine learning systems."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
